{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c42bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from atae_lstm import *\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6678de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH = '../data/absa/SemEval14/abas-pytorch/Laptops_Train.xml.seg'\n",
    "TEST_FILE_PATH = '../data/absa/SemEval14/abas-pytorch/Laptops_Test_Gold.xml.seg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f6329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad49754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer:  ./output/laptop_tokenizer.dat\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = build_tokenizer(fnames=[TRAIN_FILE_PATH, TEST_FILE_PATH], \n",
    "                            max_seq_len=85,\n",
    "                            dat_fname='./output/laptop_tokenizer.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5924b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2328\n",
      "2096\n"
     ]
    }
   ],
   "source": [
    "trainset = ABSADataset(TRAIN_FILE_PATH, tokenizer)\n",
    "testset = ABSADataset(TEST_FILE_PATH, tokenizer)\n",
    "\n",
    "print(len(trainset))\n",
    "val_len = int(len(trainset) * 0.1)\n",
    "trainset, valset = random_split(trainset, [len(trainset) - val_len, val_len])\n",
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890850bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:  [1 2 0 0]\n",
      "x2:  [11 22 33 44]\n",
      "x_batch:  tensor([[ 1,  2,  0,  0],\n",
      "        [11, 22, 33, 44]])\n",
      "tensor([[[-0.9197,  0.2485, -1.0594],\n",
      "         [-0.7178, -1.2320, -0.9252],\n",
      "         [ 0.8024,  0.1987, -0.0424],\n",
      "         [ 0.8024,  0.1987, -0.0424]],\n",
      "\n",
      "        [[-0.0301,  1.1027, -0.3387],\n",
      "         [-1.6352, -0.1453, -0.1169],\n",
      "         [-1.1660, -0.2237, -0.0090],\n",
      "         [-1.0424, -1.9349, -1.3921]]], grad_fn=<EmbeddingBackward0>)\n",
      "rnn forward....\n",
      "x raw:  tensor([[[-0.9197,  0.2485, -1.0594],\n",
      "         [-0.7178, -1.2320, -0.9252],\n",
      "         [ 0.8024,  0.1987, -0.0424],\n",
      "         [ 0.8024,  0.1987, -0.0424]],\n",
      "\n",
      "        [[-0.0301,  1.1027, -0.3387],\n",
      "         [-1.6352, -0.1453, -0.1169],\n",
      "         [-1.1660, -0.2237, -0.0090],\n",
      "         [-1.0424, -1.9349, -1.3921]]], grad_fn=<EmbeddingBackward0>)\n",
      "x len:  tensor([2, 4])\n",
      "x_sort_idx:  tensor([1, 0])\n",
      "x_sort:  tensor([[[-0.0301,  1.1027, -0.3387],\n",
      "         [-1.6352, -0.1453, -0.1169],\n",
      "         [-1.1660, -0.2237, -0.0090],\n",
      "         [-1.0424, -1.9349, -1.3921]],\n",
      "\n",
      "        [[-0.9197,  0.2485, -1.0594],\n",
      "         [-0.7178, -1.2320, -0.9252],\n",
      "         [ 0.8024,  0.1987, -0.0424],\n",
      "         [ 0.8024,  0.1987, -0.0424]]], grad_fn=<IndexBackward0>)\n",
      "x_emb_p:  PackedSequence(data=tensor([[-0.0301,  1.1027, -0.3387],\n",
      "        [-0.9197,  0.2485, -1.0594],\n",
      "        [-1.6352, -0.1453, -0.1169],\n",
      "        [-0.7178, -1.2320, -0.9252],\n",
      "        [-1.1660, -0.2237, -0.0090],\n",
      "        [-1.0424, -1.9349, -1.3921]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out_pack:  PackedSequence(data=tensor([[-0.0858, -0.0344, -0.2254, -0.1792, -0.0980, -0.0893],\n",
      "        [-0.1445, -0.0960, -0.1907, -0.0464, -0.0166, -0.0972],\n",
      "        [-0.2205, -0.0876, -0.2462, -0.2056,  0.0655,  0.0139],\n",
      "        [-0.1838, -0.1586, -0.0981,  0.0037,  0.1173, -0.0785],\n",
      "        [-0.2481, -0.1121, -0.2171, -0.2206,  0.1380,  0.0498],\n",
      "        [-0.1966, -0.1777, -0.0624, -0.0269,  0.2261,  0.0074]],\n",
      "       grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out1:  (tensor([[[-0.0858, -0.0344, -0.2254, -0.1792, -0.0980, -0.0893],\n",
      "         [-0.2205, -0.0876, -0.2462, -0.2056,  0.0655,  0.0139],\n",
      "         [-0.2481, -0.1121, -0.2171, -0.2206,  0.1380,  0.0498],\n",
      "         [-0.1966, -0.1777, -0.0624, -0.0269,  0.2261,  0.0074]],\n",
      "\n",
      "        [[-0.1445, -0.0960, -0.1907, -0.0464, -0.0166, -0.0972],\n",
      "         [-0.1838, -0.1586, -0.0981,  0.0037,  0.1173, -0.0785],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([4, 2]))\n",
      "out2:  tensor([[[-0.0858, -0.0344, -0.2254, -0.1792, -0.0980, -0.0893],\n",
      "         [-0.2205, -0.0876, -0.2462, -0.2056,  0.0655,  0.0139],\n",
      "         [-0.2481, -0.1121, -0.2171, -0.2206,  0.1380,  0.0498],\n",
      "         [-0.1966, -0.1777, -0.0624, -0.0269,  0.2261,  0.0074]],\n",
      "\n",
      "        [[-0.1445, -0.0960, -0.1907, -0.0464, -0.0166, -0.0972],\n",
      "         [-0.1838, -0.1586, -0.0981,  0.0037,  0.1173, -0.0785],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "out3:  tensor([[[-0.1445, -0.0960, -0.1907, -0.0464, -0.0166, -0.0972],\n",
      "         [-0.1838, -0.1586, -0.0981,  0.0037,  0.1173, -0.0785],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0858, -0.0344, -0.2254, -0.1792, -0.0980, -0.0893],\n",
      "         [-0.2205, -0.0876, -0.2462, -0.2056,  0.0655,  0.0139],\n",
      "         [-0.2481, -0.1121, -0.2171, -0.2206,  0.1380,  0.0498],\n",
      "         [-0.1966, -0.1777, -0.0624, -0.0269,  0.2261,  0.0074]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "ct:  tensor([[[-0.8620, -0.3415, -0.0923, -0.0452,  0.7024,  0.0139],\n",
      "         [-0.6050, -0.3087, -0.1537,  0.0061,  0.3096, -0.1576]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1 = [1, 2]\n",
    "x2 = [11, 22, 33, 44]\n",
    "x_len = [2, 4]\n",
    "x_len = torch.tensor(x_len)\n",
    "x1_pad = pad_and_truncate(x1, 4)\n",
    "x2_pad = pad_and_truncate(x2, 4)\n",
    "print('x1: ', x1_pad)\n",
    "print('x2: ', x2_pad)\n",
    "\n",
    "x_batch = np.vstack((x1_pad, x2_pad))\n",
    "x_batch = torch.tensor(x_batch)\n",
    "print('x_batch: ', x_batch)\n",
    "\n",
    "embedding = nn.Embedding(1000, 3)\n",
    "x_batch_embd = embedding(x_batch)\n",
    "print(x_batch_embd)\n",
    "\n",
    "rnn_test = DynamicRNN(3, 6)\n",
    "out, (ht, ct) = rnn_test(x_batch_embd, x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a2018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775530f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
