{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c42bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from atae_lstm import *\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6678de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH = '../data/absa/SemEval14/abas-pytorch/Laptops_Train.xml.seg'\n",
    "TEST_FILE_PATH = '../data/absa/SemEval14/abas-pytorch/Laptops_Test_Gold.xml.seg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f6329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad49754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer:  ./output/laptop_tokenizer.dat\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = build_tokenizer(fnames=[TRAIN_FILE_PATH, TEST_FILE_PATH], \n",
    "                            max_seq_len=85,\n",
    "                            dat_fname='./output/laptop_tokenizer.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5924b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2328\n",
      "2096\n"
     ]
    }
   ],
   "source": [
    "trainset = ABSADataset(TRAIN_FILE_PATH, tokenizer)\n",
    "testset = ABSADataset(TEST_FILE_PATH, tokenizer)\n",
    "\n",
    "print(len(trainset))\n",
    "val_len = int(len(trainset) * 0.1)\n",
    "trainset, valset = random_split(trainset, [len(trainset) - val_len, val_len])\n",
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "890850bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:  [1 2 0 0]\n",
      "x2:  [11 22 33 44]\n",
      "x_batch:  tensor([[ 1,  2,  0,  0],\n",
      "        [11, 22, 33, 44]])\n",
      "embedding:  tensor([[[ 0.9648,  0.5789,  0.4078],\n",
      "         [ 1.0477, -1.1492,  0.4719],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.6998,  0.5070,  0.1527],\n",
      "         [ 0.9293, -2.2551, -0.2015],\n",
      "         [ 0.6141, -0.1342, -0.8167],\n",
      "         [ 0.8165, -0.9285,  0.7614]]], grad_fn=<EmbeddingBackward0>)\n",
      "squeeze embedding:  tensor([[[ 0.9648,  0.5789,  0.4078],\n",
      "         [ 1.0477, -1.1492,  0.4719],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.6998,  0.5070,  0.1527],\n",
      "         [ 0.9293, -2.2551, -0.2015],\n",
      "         [ 0.6141, -0.1342, -0.8167],\n",
      "         [ 0.8165, -0.9285,  0.7614]]], grad_fn=<IndexBackward0>)\n",
      "attention output: tensor([[[ 0.3925, -0.7789, -0.2674]],\n",
      "\n",
      "        [[ 0.4479, -0.5102, -0.3536]]], grad_fn=<AddBackward0>), \n",
      " score: tensor([[[0.3619, 0.2396, 0.1992, 0.1992]],\n",
      "\n",
      "        [[0.1997, 0.2085, 0.3148, 0.2770]]], grad_fn=<SoftmaxBackward0>)\n",
      "start rnn forward ====>\n",
      "x raw:  tensor([[[ 0.9648,  0.5789,  0.4078],\n",
      "         [ 1.0477, -1.1492,  0.4719],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.6998,  0.5070,  0.1527],\n",
      "         [ 0.9293, -2.2551, -0.2015],\n",
      "         [ 0.6141, -0.1342, -0.8167],\n",
      "         [ 0.8165, -0.9285,  0.7614]]], grad_fn=<EmbeddingBackward0>)\n",
      "x len:  tensor([2, 4])\n",
      "x_sort_idx:  tensor([1, 0])\n",
      "x_sort:  tensor([[[-0.6998,  0.5070,  0.1527],\n",
      "         [ 0.9293, -2.2551, -0.2015],\n",
      "         [ 0.6141, -0.1342, -0.8167],\n",
      "         [ 0.8165, -0.9285,  0.7614]],\n",
      "\n",
      "        [[ 0.9648,  0.5789,  0.4078],\n",
      "         [ 1.0477, -1.1492,  0.4719],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<IndexBackward0>)\n",
      "x_emb_p:  PackedSequence(data=tensor([[-0.6998,  0.5070,  0.1527],\n",
      "        [ 0.9648,  0.5789,  0.4078],\n",
      "        [ 0.9293, -2.2551, -0.2015],\n",
      "        [ 1.0477, -1.1492,  0.4719],\n",
      "        [ 0.6141, -0.1342, -0.8167],\n",
      "        [ 0.8165, -0.9285,  0.7614]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out_pack:  PackedSequence(data=tensor([[ 0.1330,  0.0246, -0.0064,  0.2274, -0.0384, -0.1328],\n",
      "        [ 0.2023, -0.0616,  0.1404,  0.1348, -0.0079, -0.0402],\n",
      "        [-0.0226, -0.1325,  0.0741,  0.0626,  0.2288, -0.0212],\n",
      "        [ 0.1121, -0.1803,  0.1456,  0.0808,  0.1903, -0.0074],\n",
      "        [-0.0111, -0.1555,  0.1234,  0.1151,  0.0064, -0.0023],\n",
      "        [ 0.0614, -0.2053,  0.1335,  0.1077,  0.1713, -0.0029]],\n",
      "       grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out1:  (tensor([[[ 0.1330,  0.0246, -0.0064,  0.2274, -0.0384, -0.1328],\n",
      "         [-0.0226, -0.1325,  0.0741,  0.0626,  0.2288, -0.0212],\n",
      "         [-0.0111, -0.1555,  0.1234,  0.1151,  0.0064, -0.0023],\n",
      "         [ 0.0614, -0.2053,  0.1335,  0.1077,  0.1713, -0.0029]],\n",
      "\n",
      "        [[ 0.2023, -0.0616,  0.1404,  0.1348, -0.0079, -0.0402],\n",
      "         [ 0.1121, -0.1803,  0.1456,  0.0808,  0.1903, -0.0074],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([4, 2]))\n",
      "out2:  tensor([[[ 0.1330,  0.0246, -0.0064,  0.2274, -0.0384, -0.1328],\n",
      "         [-0.0226, -0.1325,  0.0741,  0.0626,  0.2288, -0.0212],\n",
      "         [-0.0111, -0.1555,  0.1234,  0.1151,  0.0064, -0.0023],\n",
      "         [ 0.0614, -0.2053,  0.1335,  0.1077,  0.1713, -0.0029]],\n",
      "\n",
      "        [[ 0.2023, -0.0616,  0.1404,  0.1348, -0.0079, -0.0402],\n",
      "         [ 0.1121, -0.1803,  0.1456,  0.0808,  0.1903, -0.0074],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "out3:  tensor([[[ 0.2023, -0.0616,  0.1404,  0.1348, -0.0079, -0.0402],\n",
      "         [ 0.1121, -0.1803,  0.1456,  0.0808,  0.1903, -0.0074],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1330,  0.0246, -0.0064,  0.2274, -0.0384, -0.1328],\n",
      "         [-0.0226, -0.1325,  0.0741,  0.0626,  0.2288, -0.0212],\n",
      "         [-0.0111, -0.1555,  0.1234,  0.1151,  0.0064, -0.0023],\n",
      "         [ 0.0614, -0.2053,  0.1335,  0.1077,  0.1713, -0.0029]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "ct:  tensor([[[ 0.1615, -0.3941,  0.1884,  0.1827,  0.2794, -0.0073],\n",
      "         [ 0.3088, -0.3445,  0.2039,  0.1299,  0.2921, -0.0180]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "<===== end rnn forward!\n"
     ]
    }
   ],
   "source": [
    "x1 = [1, 2]\n",
    "x2 = [11, 22, 33, 44]\n",
    "x_len = [2, 4]\n",
    "x_len = torch.tensor(x_len)\n",
    "x1_pad = pad_and_truncate(x1, 4)\n",
    "x2_pad = pad_and_truncate(x2, 4)\n",
    "embedding_dim = 3\n",
    "\n",
    "print('x1: ', x1_pad)\n",
    "print('x2: ', x2_pad)\n",
    "\n",
    "x_batch = np.vstack((x1_pad, x2_pad))\n",
    "x_batch = torch.tensor(x_batch)\n",
    "print('x_batch: ', x_batch)\n",
    "\n",
    "embedding = nn.Embedding(1000, embedding_dim, padding_idx=0)\n",
    "x_batch_embd = embedding(x_batch)\n",
    "print('embedding: ', x_batch_embd)\n",
    "\n",
    "squeeze_embedding = SqueezeEmbedding()\n",
    "x_batch_squeeze_embd = squeeze_embedding(x_batch_embd, x_len)\n",
    "print('squeeze embedding: ', x_batch_squeeze_embd)\n",
    "\n",
    "attn = NoQueryAttention(embedding_dim)\n",
    "output, score = attn(x_batch_squeeze_embd)\n",
    "print(f'attention output: {output}, \\n score: {score}')\n",
    "\n",
    "rnn_test = DynamicRNN(embedding_dim, 6)\n",
    "out, (ht, ct) = rnn_test(x_batch_embd, x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6da56b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch: tensor([[ 1,  2,  0,  0],\n",
      "        [11, 22, 33, 44]]), \n",
      "aspect_batch: tensor([[9, 0, 0],\n",
      "        [8, 9, 0]])\n",
      "text_indices: tensor([[ 1,  2,  0,  0],\n",
      "        [11, 22, 33, 44]]), \n",
      "aspect_indices: tensor([[9, 0, 0],\n",
      "        [8, 9, 0]])\n",
      "x_len: tensor([2, 4])\n",
      "x embed: tensor([[[-1.1631,  1.0519, -1.6253],\n",
      "         [-0.1466,  0.6123, -0.6912],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.1159,  0.4964,  0.5894],\n",
      "         [-1.0793,  0.2259,  0.1929],\n",
      "         [-0.8608, -0.1362,  0.5534],\n",
      "         [-0.4917,  1.4615,  0.6565]]], grad_fn=<IndexBackward0>)\n",
      "aspect embed: tensor([[[ 0.1481, -0.5886,  0.9991],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.5531, -0.3588,  0.0470],\n",
      "         [ 0.1481, -0.5886,  0.9991],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.sum(aspect, dim=1): tensor([[ 0.1481, -0.5886,  0.9991],\n",
      "        [ 0.7012, -0.9474,  1.0461]], grad_fn=<SumBackward1>)\n",
      "aspect_len: tensor([1., 2.]), aspect_len.unsqueeze(1): tensor([[1.],\n",
      "        [2.]])\n",
      "aspect_pool: tensor([[ 0.1481, -0.5886,  0.9991],\n",
      "        [ 0.3506, -0.4737,  0.5231]], grad_fn=<DivBackward0>)\n",
      "aspect_pool.unsqueeze(1): tensor([[[ 0.1481, -0.5886,  0.9991]],\n",
      "\n",
      "        [[ 0.3506, -0.4737,  0.5231]]], grad_fn=<UnsqueezeBackward0>)\n",
      "aspect formatted: tensor([[[ 0.1481, -0.5886,  0.9991],\n",
      "         [ 0.1481, -0.5886,  0.9991],\n",
      "         [ 0.1481, -0.5886,  0.9991],\n",
      "         [ 0.1481, -0.5886,  0.9991]],\n",
      "\n",
      "        [[ 0.3506, -0.4737,  0.5231],\n",
      "         [ 0.3506, -0.4737,  0.5231],\n",
      "         [ 0.3506, -0.4737,  0.5231],\n",
      "         [ 0.3506, -0.4737,  0.5231]]], grad_fn=<ExpandBackward0>)\n",
      "start rnn forward ====>\n",
      "x raw:  tensor([[[ 0.1481, -0.5886,  0.9991, -1.1631,  1.0519, -1.6253],\n",
      "         [ 0.1481, -0.5886,  0.9991, -0.1466,  0.6123, -0.6912],\n",
      "         [ 0.1481, -0.5886,  0.9991,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1481, -0.5886,  0.9991,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3506, -0.4737,  0.5231, -1.1159,  0.4964,  0.5894],\n",
      "         [ 0.3506, -0.4737,  0.5231, -1.0793,  0.2259,  0.1929],\n",
      "         [ 0.3506, -0.4737,  0.5231, -0.8608, -0.1362,  0.5534],\n",
      "         [ 0.3506, -0.4737,  0.5231, -0.4917,  1.4615,  0.6565]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "x len:  tensor([2, 4])\n",
      "x_sort_idx:  tensor([1, 0])\n",
      "x_sort:  tensor([[[ 0.3506, -0.4737,  0.5231, -1.1159,  0.4964,  0.5894],\n",
      "         [ 0.3506, -0.4737,  0.5231, -1.0793,  0.2259,  0.1929],\n",
      "         [ 0.3506, -0.4737,  0.5231, -0.8608, -0.1362,  0.5534],\n",
      "         [ 0.3506, -0.4737,  0.5231, -0.4917,  1.4615,  0.6565]],\n",
      "\n",
      "        [[ 0.1481, -0.5886,  0.9991, -1.1631,  1.0519, -1.6253],\n",
      "         [ 0.1481, -0.5886,  0.9991, -0.1466,  0.6123, -0.6912],\n",
      "         [ 0.1481, -0.5886,  0.9991,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1481, -0.5886,  0.9991,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "x_emb_p:  PackedSequence(data=tensor([[ 0.3506, -0.4737,  0.5231, -1.1159,  0.4964,  0.5894],\n",
      "        [ 0.1481, -0.5886,  0.9991, -1.1631,  1.0519, -1.6253],\n",
      "        [ 0.3506, -0.4737,  0.5231, -1.0793,  0.2259,  0.1929],\n",
      "        [ 0.1481, -0.5886,  0.9991, -0.1466,  0.6123, -0.6912],\n",
      "        [ 0.3506, -0.4737,  0.5231, -0.8608, -0.1362,  0.5534],\n",
      "        [ 0.3506, -0.4737,  0.5231, -0.4917,  1.4615,  0.6565]],\n",
      "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out_pack:  PackedSequence(data=tensor([[ 0.2021, -0.0672,  0.0285, -0.1085, -0.1381,  0.0178],\n",
      "        [ 0.1076, -0.1502,  0.0973, -0.2539, -0.0977,  0.0084],\n",
      "        [ 0.2667, -0.0782,  0.0499, -0.1674, -0.1983,  0.0502],\n",
      "        [ 0.1496, -0.1715,  0.1380, -0.2910, -0.2684,  0.0643],\n",
      "        [ 0.2952, -0.0291,  0.0490, -0.1486, -0.2437,  0.0942],\n",
      "        [ 0.2515, -0.1369,  0.1057, -0.1966, -0.3986,  0.0764]],\n",
      "       grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out1:  (tensor([[[ 0.2021, -0.0672,  0.0285, -0.1085, -0.1381,  0.0178],\n",
      "         [ 0.2667, -0.0782,  0.0499, -0.1674, -0.1983,  0.0502],\n",
      "         [ 0.2952, -0.0291,  0.0490, -0.1486, -0.2437,  0.0942],\n",
      "         [ 0.2515, -0.1369,  0.1057, -0.1966, -0.3986,  0.0764]],\n",
      "\n",
      "        [[ 0.1076, -0.1502,  0.0973, -0.2539, -0.0977,  0.0084],\n",
      "         [ 0.1496, -0.1715,  0.1380, -0.2910, -0.2684,  0.0643],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([4, 2]))\n",
      "out2:  tensor([[[ 0.2021, -0.0672,  0.0285, -0.1085, -0.1381,  0.0178],\n",
      "         [ 0.2667, -0.0782,  0.0499, -0.1674, -0.1983,  0.0502],\n",
      "         [ 0.2952, -0.0291,  0.0490, -0.1486, -0.2437,  0.0942],\n",
      "         [ 0.2515, -0.1369,  0.1057, -0.1966, -0.3986,  0.0764]],\n",
      "\n",
      "        [[ 0.1076, -0.1502,  0.0973, -0.2539, -0.0977,  0.0084],\n",
      "         [ 0.1496, -0.1715,  0.1380, -0.2910, -0.2684,  0.0643],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "out3:  tensor([[[ 0.1076, -0.1502,  0.0973, -0.2539, -0.0977,  0.0084],\n",
      "         [ 0.1496, -0.1715,  0.1380, -0.2910, -0.2684,  0.0643],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2021, -0.0672,  0.0285, -0.1085, -0.1381,  0.0178],\n",
      "         [ 0.2667, -0.0782,  0.0499, -0.1674, -0.1983,  0.0502],\n",
      "         [ 0.2952, -0.0291,  0.0490, -0.1486, -0.2437,  0.0942],\n",
      "         [ 0.2515, -0.1369,  0.1057, -0.1966, -0.3986,  0.0764]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "ct:  tensor([[[ 0.6695, -0.2633,  0.3932, -0.2612, -0.6203,  0.2727],\n",
      "         [ 0.4376, -0.3168,  0.4180, -0.4248, -0.4365,  0.1642]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "<===== end rnn forward!\n",
      "lstm h size: torch.Size([2, 4, 6]), \n",
      "ha size: torch.Size([2, 4, 9])\n",
      "score : tensor([[[0.2523, 0.2531, 0.2473, 0.2473]],\n",
      "\n",
      "        [[0.2492, 0.2500, 0.2493, 0.2514]]], grad_fn=<SoftmaxBackward0>)\n",
      "score_h size: torch.Size([2, 1, 6])\n",
      "output size: torch.Size([2, 6])\n",
      "final_out: tensor([[-0.4255, -0.3066, -0.0104],\n",
      "        [-0.5007, -0.2817,  0.0051]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "embed_dim = 3\n",
    "hidden_dim = 6\n",
    "\n",
    "x1 = [1, 2]\n",
    "x2 = [11, 22, 33, 44]\n",
    "aspect1 = [9]\n",
    "aspect2 = [8, 9]\n",
    "\n",
    "x1_pad = pad_and_truncate(x1, 4)\n",
    "x2_pad = pad_and_truncate(x2, 4)\n",
    "aspect1_pad = pad_and_truncate(aspect1, 3)\n",
    "aspect2_pad = pad_and_truncate(aspect2, 3)\n",
    "\n",
    "x_batch = torch.tensor(np.vstack((x1_pad, x2_pad)))\n",
    "aspect_batch = torch.tensor(np.vstack((aspect1_pad, aspect2_pad)))\n",
    "print(f'x_batch: {x_batch}, \\naspect_batch: {aspect_batch}')\n",
    "\n",
    "\n",
    "atae = ATAE_LSTM(embed_dim, vocab_size, hidden_dim)\n",
    "final_out = atae((x_batch, aspect_batch))\n",
    "print(f'final_out: {final_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941616a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc493e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1e131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
