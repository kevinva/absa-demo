{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c42bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from atae_lstm import *\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6678de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH = '../data/absa/SemEval14/abas-pytorch/Laptops_Train.xml.seg'\n",
    "TEST_FILE_PATH = '../data/absa/SemEval14/abas-pytorch/Laptops_Test_Gold.xml.seg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f6329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad49754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer:  ./output/laptop_tokenizer.dat\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = build_tokenizer(fnames=[TRAIN_FILE_PATH, TEST_FILE_PATH], \n",
    "                            max_seq_len=85,\n",
    "                            dat_fname='./output/laptop_tokenizer.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ABSADataset(TRAIN_FILE_PATH, tokenizer)\n",
    "testset = ABSADataset(TEST_FILE_PATH, tokenizer)\n",
    "\n",
    "print(len(trainset))\n",
    "val_len = int(len(trainset) * 0.1)\n",
    "trainset, valset = random_split(trainset, [len(trainset) - val_len, val_len])\n",
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "890850bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:  [1 2 0 0]\n",
      "x2:  [11 22 33 44]\n",
      "x_batch:  tensor([[ 1,  2,  0,  0],\n",
      "        [11, 22, 33, 44]])\n",
      "embedding:  tensor([[[-1.8584, -0.3601, -0.0726],\n",
      "         [ 1.7019, -0.2690, -0.4833],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.4000,  0.2280, -0.0531],\n",
      "         [ 0.9400, -0.1664, -0.0234],\n",
      "         [ 0.3305, -0.7436, -0.6359],\n",
      "         [-0.1622, -0.1226, -1.1903]]], grad_fn=<EmbeddingBackward0>)\n",
      "squeeze embedding:  tensor([[[-1.8584, -0.3601, -0.0726],\n",
      "         [ 1.7019, -0.2690, -0.4833],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.4000,  0.2280, -0.0531],\n",
      "         [ 0.9400, -0.1664, -0.0234],\n",
      "         [ 0.3305, -0.7436, -0.6359],\n",
      "         [-0.1622, -0.1226, -1.1903]]], grad_fn=<IndexBackward0>)\n",
      "attention output: tensor([[[0.4671, 0.2107, 0.1577]],\n",
      "\n",
      "        [[0.4889, 0.0986, 0.1362]]], grad_fn=<AddBackward0>), \n",
      " score: tensor([[[0.2413, 0.2558, 0.2515, 0.2515]],\n",
      "\n",
      "        [[0.2515, 0.2533, 0.2459, 0.2493]]], grad_fn=<SoftmaxBackward0>)\n",
      "rnn forward....\n",
      "x raw:  tensor([[[-1.8584, -0.3601, -0.0726],\n",
      "         [ 1.7019, -0.2690, -0.4833],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.4000,  0.2280, -0.0531],\n",
      "         [ 0.9400, -0.1664, -0.0234],\n",
      "         [ 0.3305, -0.7436, -0.6359],\n",
      "         [-0.1622, -0.1226, -1.1903]]], grad_fn=<EmbeddingBackward0>)\n",
      "x len:  tensor([2, 4])\n",
      "x_sort_idx:  tensor([1, 0])\n",
      "x_sort:  tensor([[[-0.4000,  0.2280, -0.0531],\n",
      "         [ 0.9400, -0.1664, -0.0234],\n",
      "         [ 0.3305, -0.7436, -0.6359],\n",
      "         [-0.1622, -0.1226, -1.1903]],\n",
      "\n",
      "        [[-1.8584, -0.3601, -0.0726],\n",
      "         [ 1.7019, -0.2690, -0.4833],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<IndexBackward0>)\n",
      "x_emb_p:  PackedSequence(data=tensor([[-0.4000,  0.2280, -0.0531],\n",
      "        [-1.8584, -0.3601, -0.0726],\n",
      "        [ 0.9400, -0.1664, -0.0234],\n",
      "        [ 1.7019, -0.2690, -0.4833],\n",
      "        [ 0.3305, -0.7436, -0.6359],\n",
      "        [-0.1622, -0.1226, -1.1903]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out_pack:  PackedSequence(data=tensor([[-0.0169,  0.0883,  0.0431,  0.0594,  0.2091,  0.0320],\n",
      "        [-0.0816,  0.1753,  0.0813,  0.0974,  0.2693,  0.1252],\n",
      "        [ 0.1113,  0.0604, -0.0271, -0.0576,  0.0786, -0.0483],\n",
      "        [ 0.1175,  0.0069, -0.0375, -0.0893,  0.0013, -0.0441],\n",
      "        [ 0.1445,  0.0767, -0.0374, -0.1149,  0.1043, -0.0939],\n",
      "        [ 0.0509,  0.0375, -0.0109, -0.1041,  0.3206, -0.1254]],\n",
      "       grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out1:  (tensor([[[-0.0169,  0.0883,  0.0431,  0.0594,  0.2091,  0.0320],\n",
      "         [ 0.1113,  0.0604, -0.0271, -0.0576,  0.0786, -0.0483],\n",
      "         [ 0.1445,  0.0767, -0.0374, -0.1149,  0.1043, -0.0939],\n",
      "         [ 0.0509,  0.0375, -0.0109, -0.1041,  0.3206, -0.1254]],\n",
      "\n",
      "        [[-0.0816,  0.1753,  0.0813,  0.0974,  0.2693,  0.1252],\n",
      "         [ 0.1175,  0.0069, -0.0375, -0.0893,  0.0013, -0.0441],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([4, 2]))\n",
      "out2:  tensor([[[-0.0169,  0.0883,  0.0431,  0.0594,  0.2091,  0.0320],\n",
      "         [ 0.1113,  0.0604, -0.0271, -0.0576,  0.0786, -0.0483],\n",
      "         [ 0.1445,  0.0767, -0.0374, -0.1149,  0.1043, -0.0939],\n",
      "         [ 0.0509,  0.0375, -0.0109, -0.1041,  0.3206, -0.1254]],\n",
      "\n",
      "        [[-0.0816,  0.1753,  0.0813,  0.0974,  0.2693,  0.1252],\n",
      "         [ 0.1175,  0.0069, -0.0375, -0.0893,  0.0013, -0.0441],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "out3:  tensor([[[-0.0816,  0.1753,  0.0813,  0.0974,  0.2693,  0.1252],\n",
      "         [ 0.1175,  0.0069, -0.0375, -0.0893,  0.0013, -0.0441],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0169,  0.0883,  0.0431,  0.0594,  0.2091,  0.0320],\n",
      "         [ 0.1113,  0.0604, -0.0271, -0.0576,  0.0786, -0.0483],\n",
      "         [ 0.1445,  0.0767, -0.0374, -0.1149,  0.1043, -0.0939],\n",
      "         [ 0.0509,  0.0375, -0.0109, -0.1041,  0.3206, -0.1254]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "ct:  tensor([[[ 0.1410,  0.0733, -0.0342, -0.2228,  0.4550, -0.2888],\n",
      "         [ 0.3263,  0.0146, -0.1019, -0.1469,  0.0021, -0.1092]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1 = [1, 2]\n",
    "x2 = [11, 22, 33, 44]\n",
    "x_len = [2, 4]\n",
    "x_len = torch.tensor(x_len)\n",
    "x1_pad = pad_and_truncate(x1, 4)\n",
    "x2_pad = pad_and_truncate(x2, 4)\n",
    "embedding_dim = 3\n",
    "\n",
    "print('x1: ', x1_pad)\n",
    "print('x2: ', x2_pad)\n",
    "\n",
    "x_batch = np.vstack((x1_pad, x2_pad))\n",
    "x_batch = torch.tensor(x_batch)\n",
    "print('x_batch: ', x_batch)\n",
    "\n",
    "embedding = nn.Embedding(1000, embedding_dim, padding_idx=0)\n",
    "x_batch_embd = embedding(x_batch)\n",
    "print('embedding: ', x_batch_embd)\n",
    "\n",
    "squeeze_embedding = SqueezeEmbedding()\n",
    "x_batch_squeeze_embd = squeeze_embedding(x_batch_embd, x_len)\n",
    "print('squeeze embedding: ', x_batch_squeeze_embd)\n",
    "\n",
    "attn = NoQueryAttention(embedding_dim)\n",
    "output, score = attn(x_batch_squeeze_embd)\n",
    "print(f'attention output: {output}, \\n score: {score}')\n",
    "\n",
    "rnn_test = DynamicRNN(embedding_dim, 6)\n",
    "out, (ht, ct) = rnn_test(x_batch_embd, x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6da56b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch: tensor([[ 1,  2,  0,  0],\n",
      "        [11, 22, 33, 44]]), \n",
      "aspect_batch: tensor([[9, 0, 0],\n",
      "        [8, 9, 0]])\n",
      "text_indices: tensor([[ 1,  2,  0,  0],\n",
      "        [11, 22, 33, 44]]), \n",
      "aspect_indices: tensor([[9, 0, 0],\n",
      "        [8, 9, 0]])\n",
      "x_len: tensor([2, 4])\n",
      "x embed: tensor([[[ 1.7366,  0.8752, -1.0643],\n",
      "         [ 0.6609,  1.9008, -1.6762],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1033,  1.3022,  0.4987],\n",
      "         [-0.7301,  0.4664,  2.2021],\n",
      "         [ 0.0893, -0.5834, -2.1886],\n",
      "         [-3.0773,  0.1663, -3.0898]]], grad_fn=<IndexBackward0>)\n",
      "aspect embed: tensor([[[-0.7560, -0.6563,  1.7949],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.5416,  0.0565,  0.8412],\n",
      "         [-0.7560, -0.6563,  1.7949],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.sum(aspect, dim=1): tensor([[-0.7560, -0.6563,  1.7949],\n",
      "        [-2.2976, -0.5998,  2.6361]], grad_fn=<SumBackward1>)\n",
      "aspect_len: tensor([1., 2.]), aspect_len.unsqueeze(1): tensor([[1.],\n",
      "        [2.]])\n",
      "aspect_pool: tensor([[-0.7560, -0.6563,  1.7949],\n",
      "        [-1.1488, -0.2999,  1.3180]], grad_fn=<DivBackward0>)\n",
      "aspect_pool.unsqueeze(1): tensor([[[-0.7560, -0.6563,  1.7949]],\n",
      "\n",
      "        [[-1.1488, -0.2999,  1.3180]]], grad_fn=<UnsqueezeBackward0>)\n",
      "aspect formatted: tensor([[[-0.7560, -0.6563,  1.7949],\n",
      "         [-0.7560, -0.6563,  1.7949],\n",
      "         [-0.7560, -0.6563,  1.7949],\n",
      "         [-0.7560, -0.6563,  1.7949]],\n",
      "\n",
      "        [[-1.1488, -0.2999,  1.3180],\n",
      "         [-1.1488, -0.2999,  1.3180],\n",
      "         [-1.1488, -0.2999,  1.3180],\n",
      "         [-1.1488, -0.2999,  1.3180]]], grad_fn=<ExpandBackward0>)\n",
      "rnn forward....\n",
      "x raw:  tensor([[[-0.7560, -0.6563,  1.7949,  1.7366,  0.8752, -1.0643],\n",
      "         [-0.7560, -0.6563,  1.7949,  0.6609,  1.9008, -1.6762],\n",
      "         [-0.7560, -0.6563,  1.7949,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7560, -0.6563,  1.7949,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.1488, -0.2999,  1.3180,  0.1033,  1.3022,  0.4987],\n",
      "         [-1.1488, -0.2999,  1.3180, -0.7301,  0.4664,  2.2021],\n",
      "         [-1.1488, -0.2999,  1.3180,  0.0893, -0.5834, -2.1886],\n",
      "         [-1.1488, -0.2999,  1.3180, -3.0773,  0.1663, -3.0898]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "x len:  tensor([2, 4])\n",
      "x_sort_idx:  tensor([1, 0])\n",
      "x_sort:  tensor([[[-1.1488, -0.2999,  1.3180,  0.1033,  1.3022,  0.4987],\n",
      "         [-1.1488, -0.2999,  1.3180, -0.7301,  0.4664,  2.2021],\n",
      "         [-1.1488, -0.2999,  1.3180,  0.0893, -0.5834, -2.1886],\n",
      "         [-1.1488, -0.2999,  1.3180, -3.0773,  0.1663, -3.0898]],\n",
      "\n",
      "        [[-0.7560, -0.6563,  1.7949,  1.7366,  0.8752, -1.0643],\n",
      "         [-0.7560, -0.6563,  1.7949,  0.6609,  1.9008, -1.6762],\n",
      "         [-0.7560, -0.6563,  1.7949,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7560, -0.6563,  1.7949,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "x_emb_p:  PackedSequence(data=tensor([[-1.1488, -0.2999,  1.3180,  0.1033,  1.3022,  0.4987],\n",
      "        [-0.7560, -0.6563,  1.7949,  1.7366,  0.8752, -1.0643],\n",
      "        [-1.1488, -0.2999,  1.3180, -0.7301,  0.4664,  2.2021],\n",
      "        [-0.7560, -0.6563,  1.7949,  0.6609,  1.9008, -1.6762],\n",
      "        [-1.1488, -0.2999,  1.3180,  0.0893, -0.5834, -2.1886],\n",
      "        [-1.1488, -0.2999,  1.3180, -3.0773,  0.1663, -3.0898]],\n",
      "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out_pack:  PackedSequence(data=tensor([[ 0.1410, -0.0795,  0.0696,  0.0979, -0.0177,  0.0304],\n",
      "        [ 0.1718, -0.0294,  0.0957,  0.0900, -0.0531, -0.0758],\n",
      "        [ 0.0812, -0.1737, -0.0463,  0.1232,  0.1582,  0.0552],\n",
      "        [ 0.2575, -0.0360,  0.1355,  0.2145, -0.0849, -0.0587],\n",
      "        [ 0.1836,  0.0357, -0.0082,  0.2167, -0.0426, -0.1243],\n",
      "        [ 0.2969,  0.2612, -0.0176,  0.5270, -0.1486, -0.1559]],\n",
      "       grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "out1:  (tensor([[[ 0.1410, -0.0795,  0.0696,  0.0979, -0.0177,  0.0304],\n",
      "         [ 0.0812, -0.1737, -0.0463,  0.1232,  0.1582,  0.0552],\n",
      "         [ 0.1836,  0.0357, -0.0082,  0.2167, -0.0426, -0.1243],\n",
      "         [ 0.2969,  0.2612, -0.0176,  0.5270, -0.1486, -0.1559]],\n",
      "\n",
      "        [[ 0.1718, -0.0294,  0.0957,  0.0900, -0.0531, -0.0758],\n",
      "         [ 0.2575, -0.0360,  0.1355,  0.2145, -0.0849, -0.0587],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([4, 2]))\n",
      "out2:  tensor([[[ 0.1410, -0.0795,  0.0696,  0.0979, -0.0177,  0.0304],\n",
      "         [ 0.0812, -0.1737, -0.0463,  0.1232,  0.1582,  0.0552],\n",
      "         [ 0.1836,  0.0357, -0.0082,  0.2167, -0.0426, -0.1243],\n",
      "         [ 0.2969,  0.2612, -0.0176,  0.5270, -0.1486, -0.1559]],\n",
      "\n",
      "        [[ 0.1718, -0.0294,  0.0957,  0.0900, -0.0531, -0.0758],\n",
      "         [ 0.2575, -0.0360,  0.1355,  0.2145, -0.0849, -0.0587],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "out3:  tensor([[[ 0.1718, -0.0294,  0.0957,  0.0900, -0.0531, -0.0758],\n",
      "         [ 0.2575, -0.0360,  0.1355,  0.2145, -0.0849, -0.0587],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1410, -0.0795,  0.0696,  0.0979, -0.0177,  0.0304],\n",
      "         [ 0.0812, -0.1737, -0.0463,  0.1232,  0.1582,  0.0552],\n",
      "         [ 0.1836,  0.0357, -0.0082,  0.2167, -0.0426, -0.1243],\n",
      "         [ 0.2969,  0.2612, -0.0176,  0.5270, -0.1486, -0.1559]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "ct:  tensor([[[ 0.4909,  0.9731, -0.1263,  0.7083, -0.4282, -0.3779],\n",
      "         [ 0.5739, -0.2535,  0.3256,  0.5336, -0.3530, -0.1192]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "h sizer: torch.Size([2, 4, 6]), ha size: torch.Size([2, 4, 9])\n",
      "score : tensor([[[0.2496, 0.2491, 0.2507, 0.2507]],\n",
      "\n",
      "        [[0.2467, 0.2552, 0.2501, 0.2479]]], grad_fn=<SoftmaxBackward0>), size: torch.Size([2, 1, 4])\n",
      "h_score size: torch.Size([2, 1, 6])\n",
      "output size: torch.Size([2, 6])\n",
      "final_out: tensor([[ 0.2972, -0.2598, -0.3021],\n",
      "        [ 0.3041, -0.2391, -0.2326]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "embed_dim = 3\n",
    "hidden_dim = 6\n",
    "\n",
    "x1 = [1, 2]\n",
    "x2 = [11, 22, 33, 44]\n",
    "aspect1 = [9]\n",
    "aspect2 = [8, 9]\n",
    "\n",
    "x1_pad = pad_and_truncate(x1, 4)\n",
    "x2_pad = pad_and_truncate(x2, 4)\n",
    "aspect1_pad = pad_and_truncate(aspect1, 3)\n",
    "aspect2_pad = pad_and_truncate(aspect2, 3)\n",
    "\n",
    "x_batch = torch.tensor(np.vstack((x1_pad, x2_pad)))\n",
    "aspect_batch = torch.tensor(np.vstack((aspect1_pad, aspect2_pad)))\n",
    "\n",
    "print(f'x_batch: {x_batch}, \\naspect_batch: {aspect_batch}')\n",
    "\n",
    "\n",
    "atae = ATAE_LSTM(embed_dim, vocab_size, hidden_dim)\n",
    "final_out = atae((x_batch, aspect_batch))\n",
    "print(f'final_out: {final_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db39a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d41669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f28ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
